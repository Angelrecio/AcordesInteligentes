{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20d2422-9b2b-4597-a179-35d41c808090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2024 21:53:16] \"OPTIONS /procesar_video HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "import pyktok as pyk\n",
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from demucs.apply import apply_model\n",
    "from demucs import pretrained\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#-------------------Descargar videos-----------------------------------\n",
    "\n",
    "def descargar_video(url):\n",
    "    if \"youtu\" in url:\n",
    "        logging.debug(\"Es YouTube\")\n",
    "        try:\n",
    "            video = YouTube(url)\n",
    "            video.streams.first().download()\n",
    "            nombre_archivo = video.title + \".mp4\"\n",
    "            return nombre_archivo\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al descargar el video de YouTube: {e}\")\n",
    "            return None\n",
    "    elif \"tiktok\" in url:\n",
    "        try:\n",
    "            logging.debug(\"Es TikTok\")\n",
    "            # Descargar un video de TikTok y su metadato\n",
    "            pyk.save_tiktok(url, True, 'video_data.csv', 'chrome')\n",
    "            # Listar archivos descargados\n",
    "            archivos_descargados = os.listdir()\n",
    "            # Filtrar archivos con extensión .mp4\n",
    "            archivos_mp4 = [archivo for archivo in archivos_descargados if archivo.endswith('.mp4')]\n",
    "            if archivos_mp4:\n",
    "                # Tomar el primer archivo mp4 encontrado\n",
    "                nombre_archivo = archivos_mp4[0]\n",
    "                return nombre_archivo\n",
    "            else:\n",
    "                logging.error(\"No se encontraron archivos MP4 descargados.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al descargar el video de TikTok: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(\"Enlace no reconocido\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#----------------Mp4 a wav-------------------\n",
    "\n",
    "def mp4_to_wav(input_file):\n",
    "    output_file = os.path.splitext(input_file)[0] + \".wav\"\n",
    "    try:\n",
    "        video = VideoFileClip(input_file)\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(output_file)\n",
    "        video.close()\n",
    "        time.sleep(1)\n",
    "        os.remove(input_file)\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al convertir {input_file} a WAV: {e}\")\n",
    "        return None\n",
    "\n",
    "#----------------Extraer letra -------------------\n",
    "\n",
    "def extraerletra(audio):\n",
    "    import torch\n",
    "    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "    \n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=128,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,\n",
    "        return_timestamps=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "    sample = dataset[0][\"audio\"]\n",
    "    \n",
    "    result = pipe(audio)\n",
    "    return(result[\"text\"])\n",
    "\n",
    "#----------------Aislar audio-----------------\n",
    "\n",
    "def aislar(audio_file):\n",
    "    try:\n",
    "        # Cargar el archivo de audio WAV\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        \n",
    "        # Asegurarse de que el archivo tenga el formato correcto (2 canales y frecuencia de muestreo de 44100 Hz)\n",
    "        if waveform.shape[0] != 2 or sample_rate != 44100:\n",
    "            raise ValueError(\"El archivo WAV debe tener 2 canales y una frecuencia de muestreo de 44100 Hz.\")\n",
    "        \n",
    "        # Normalizar la forma de onda si es necesario\n",
    "        waveform = waveform / torch.max(torch.abs(waveform))\n",
    "        \n",
    "        # Convertir la forma de onda en tensores de PyTorch y agregar una dimensión para el lote\n",
    "        x = waveform.unsqueeze(0)\n",
    "        \n",
    "        # Cargar el modelo DEMUCS pre-entrenado\n",
    "        model = pretrained.get_model('mdx')\n",
    "        \n",
    "        # Aplicar el modelo al archivo de audio\n",
    "        out = apply_model(model, x)[0]  # El resultado tiene forma [S, C, T], donde S es el número de fuentes\n",
    "        \n",
    "        # Sumar las fuentes, ignorando \"vocals\" si está presente\n",
    "        output_waveform = torch.zeros_like(waveform)\n",
    "        for i, source in enumerate(out):\n",
    "            if model.sources[i] != \"vocals\":\n",
    "                output_waveform += source\n",
    "        \n",
    "        # Sobrescribir el archivo original con el audio resultante sin la voz humana\n",
    "        torchaudio.save(audio_file, output_waveform, sample_rate)\n",
    "        \n",
    "        return audio_file\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al aislar audio {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "#----------------Predecir acordes-----------------\n",
    "\n",
    "svm_chord = joblib.load('svm_chord_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "svm_style = joblib.load('svm_style_model.pkl')\n",
    "\n",
    "def dividir_audio_largo(ruta_audio, duracion_segmento=2, max_length=1000):\n",
    "    audio, sr = librosa.load(ruta_audio, sr=None)\n",
    "    duracion_total = librosa.get_duration(y=audio, sr=sr)\n",
    "    num_segmentos = int(np.ceil(duracion_total / duracion_segmento))\n",
    "    \n",
    "    segmentos = []\n",
    "    segmentos_caracteristicas = []\n",
    "    \n",
    "    for i in range(num_segmentos):\n",
    "        inicio = i * duracion_segmento\n",
    "        fin = min((i + 1) * duracion_segmento, duracion_total)\n",
    "        segmento_actual = audio[int(inicio * sr):int(fin * sr)]\n",
    "        \n",
    "        spectrograma = librosa.feature.melspectrogram(y=segmento_actual, sr=sr)\n",
    "        if spectrograma.shape[1] < max_length:\n",
    "            pad_width = max_length - spectrograma.shape[1]\n",
    "            spectrograma = np.pad(spectrograma, pad_width=((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            spectrograma = spectrograma[:, :max_length]\n",
    "        caracteristicas_segmento = librosa.power_to_db(spectrograma, ref=np.max).flatten()\n",
    "        \n",
    "        segmentos.append(segmento_actual)\n",
    "        segmentos_caracteristicas.append(caracteristicas_segmento)\n",
    "    \n",
    "    return segmentos, segmentos_caracteristicas\n",
    "\n",
    "def arraypredicciones(audio_file):\n",
    "    segmentos, segmentos_caracteristicas = dividir_audio_largo(audio_file, 2)\n",
    "    acordes = []\n",
    "    \n",
    "    for caracteristicas_segmento in segmentos_caracteristicas:\n",
    "        caracteristicas_segmento_escaladas = scaler.transform(caracteristicas_segmento.reshape(1, -1))\n",
    "        acorde_predicho = svm_chord.predict(caracteristicas_segmento_escaladas)\n",
    "        estilo_predicho = svm_style.predict(caracteristicas_segmento_escaladas)\n",
    "        \n",
    "        # Convertir los resultados a listas para que sean serializables en JSON\n",
    "        acordes.append((acorde_predicho[0].item(), estilo_predicho[0].item()))\n",
    "    \n",
    "    return acordes\n",
    "\n",
    "\n",
    "#----------------API Endpoint-----------------\n",
    "\n",
    "@app.route('/procesar_video', methods=['POST'])\n",
    "def procesar_video():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'url' not in data:\n",
    "            raise ValueError(\"Falta 'url' en el JSON enviado\")\n",
    "        url = data['url']\n",
    "        # Descargar video\n",
    "        video_file = descargar_video(url)\n",
    "        if not video_file:\n",
    "            logging.error(\"Error al descargar el video\")\n",
    "            return jsonify({\"error\": \"Error al descargar el video\"}), 500\n",
    "        \n",
    "        logging.debug(f\"Video descargado: {video_file}\")\n",
    "        \n",
    "        # Convertir MP4 a WAV\n",
    "        audio_file = mp4_to_wav(video_file)\n",
    "        if not audio_file:\n",
    "            logging.error(\"Error al convertir el video a WAV\")\n",
    "            return jsonify({\"error\": \"Error al convertir el video a WAV\"}), 500\n",
    "        \n",
    "        logging.debug(f\"Audio convertido: {audio_file}\")\n",
    "        \n",
    "        # Extraer letra\n",
    "        letra = extraerletra(audio_file)\n",
    "        if not letra:\n",
    "            logging.error(\"Error al extraer la letra\")\n",
    "            return jsonify({\"error\": \"Error al extraer la letra\"}), 500\n",
    "        \n",
    "        logging.debug(f\"Letra extraída: {letra}\")\n",
    "        \n",
    "        # Aislar audio\n",
    "        audio_file = aislar(audio_file)\n",
    "        time.sleep(1)\n",
    "        # Predecir acordes\n",
    "        acordes = arraypredicciones(audio_file)\n",
    "        if not acordes:\n",
    "            logging.error(\"Error al predecir los acordes\")\n",
    "            return jsonify({\"error\": \"Error al predecir los acordes\"}), 500\n",
    "        \n",
    "        logging.debug(f\"Acordes predichos: {acordes}\")\n",
    "        \n",
    "        # Eliminar archivo de audio temporal\n",
    "        if os.path.exists(audio_file):\n",
    "            os.remove(audio_file)\n",
    "    \n",
    "        response = {\"status\": \"success\", \"message\": \"URL procesada correctamente\", \"url\": url}\n",
    "        return jsonify(response), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "\n",
    "# Ejecución del servidor Flask en un hilo separado\n",
    "def run_flask():\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_flask)\n",
    "flask_thread.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f87bfe-4f7b-4e09-99c5-5dd7a168652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no está importado el modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45272a92-0ff2-4b9b-9fa5-3af07b4494f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
